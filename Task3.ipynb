{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task03：逻辑回归（2天）\n",
    "理论部分：\n",
    "\n",
    "逻辑回归与线性回归的联系与区别：\n",
    "联系：逻辑回归在本质上还是线性回归的一种特例、变形，它是在线性回归的基础上，在特征到结果的映射中加了一层sigmoid函数（非线性）映射，即先把特征求和，然后使用sigmoid函数来预测。\n",
    "即：\n",
    "$$sigmoid(f(x))=sigmoid(\\omega_1x_1 + \\omega_2x_2 + ... + \\omega_nx_n + b_i)$$\n",
    "$$sigmid(x)=\\frac{1}{1 + e^{-x\\theta^T}}$$\n",
    "他们都属于广义线性回归。\n",
    "区别：线性回归属于拟合，逻辑回归做的是分类。\n",
    "线性回归的输入到输出是线性变换，拟合能力相对差，逻辑回顾在线性的基础上，添加了非线性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型建立：逻辑回归原理:逻辑回归模型:\n",
    "（1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。\n",
    "\n",
    "（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。\n",
    "\n",
    "（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。\n",
    "学习策略：逻辑回归损失函数、推导及优化\n",
    "算法求解：批量梯度下降\n",
    "批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。从数学上理解如下：\n",
    "  （1）对目标函数求偏导：\n",
    "$$\\frac{\\Delta J(\\theta_0,\\theta_1)}{\\Delta \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "  其中 $i=1,2,...,m$表示样本数，$j = 0,1$ 表示特征数，这里我们使用了偏置项$x_0^{(i)} = 1$ 。\n",
    "  \n",
    "  （2）每次迭代对参数进行更新：\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "  注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。\n",
    "  伪代码形式为：\n",
    "  repeat{\n",
    "       $$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "      (for j =0,1)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化与模型评估指标\n",
    "正则化的思想是，前面的参数会使得函数变得很大，如果想要最小化整个函数的话，那么正则化部分的必须要小才能满足要求（可以将压缩到接近0）。一般正则化不对增加惩罚项，只对1到n，只是约定俗成的，就算对0惩罚也没有什么影响。一般我们不知道是哪个参数导致过拟合，所以我们惩罚所有的参数。\n",
    "\n",
    "L1正则化\n",
    "$$L=E_{in}+\\lambda\\sum_j|w_j|$$\n",
    "L2正则化\n",
    "$$L=E_{in}+\\lambda\\sum_jw_j^2$$\n",
    "逻辑回归的优缺点\n",
    "是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。\n",
    "优点:\n",
    "1. 实现简单，广泛的应用于工业问题上；\n",
    "2. 分类时计算量非常小，速度很快，存储资源低；\n",
    "3. 便利的观测样本概率分数；\n",
    "4. 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；\n",
    "5. 计算代价不高，易于理解和实现。\n",
    "缺点:\n",
    "1. 当特征空间很大时，逻辑回归的性能不是很好；\n",
    "2. 容易欠拟合，一般准确度不太高；\n",
    "3. 不能很好地处理大量多类特征或变量；\n",
    "4. 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可\n",
    "样本不均衡问题：\n",
    "欠采样（可能会出现欠拟合）和过采样（可能会出现过拟合）\n",
    "解决这个问题，\n",
    "方法一，模型融合 （bagging的思想 ）：从丰富类样本中随机的选取（有放回的选取）和稀有类等量样本的数据。和稀有类样本组合成新的训练集。这样我们就产生了多个训练集，并且是互相独立的，然后训练得到多个分类器。\n",
    "若是分类问题，就把多个分类器投票的结果（少数服从多数）作为分类结果。\n",
    "若是回归问题，就将均值作为最后结果。\n",
    "方法二，增量模型 （boosting的思想）：\n",
    "思路：使用全部的样本作为训练集，得到分类器L1，从L1正确分类的样本中和错误分类的样本中各抽取50%的数据，即循环的一边采样一个。此时训练样本是平衡的。训练得到的分类器作为L2.\n",
    "从L1和L2分类结果中，选取结果不一致的样本作为训练集得到分类器L3.\n",
    "最后投票L1,L2,L3结果得到最后的分类结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn参数详解\n",
    "Sklearn.linear_model.LogisticRegression\n",
    "\n",
    "(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True,intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’,verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    " \n",
    "\n",
    "penalty:’l1’ or ‘l2’ ,默认’l2’ #惩罚\n",
    "\n",
    "dual:bool 默认False ‘双配方仅用于利用liblinear解算器的l2惩罚。’\n",
    "\n",
    "tol: float, 默认: 1e-4 ‘公差停止标准’\n",
    "\n",
    "C:float 默认:1.0 正则化强度， 与支持向量机一样，较小的值指定更强的正则化。\n",
    "\n",
    "fit_intercept: bool 默认:True 指定是否应将常量（a.k.a. bias或intercept）添加到决策函数中。\n",
    "\n",
    "intercept_scaling:float ,默认:1 仅在使用求解器“liblinear”且self.fit_intercept设置为True时有用。 在这种情况下，x变为[x，self.intercept_scaling]，即具有等于intercept_scaling的常数值的“合成”特征被附加到实例矢量。 截距变为intercept_scaling * synthetic_feature_weight\n",
    "\n",
    "class_weight: dict or ‘balanced’ 默认:None\n",
    "\n",
    " 与{class_label：weight}形式的类相关联的权重。 如果没有给出，所有类都应该有一个权重。“平衡”模式使用y的值自动调整与输入数据中的类频率成反比的权重，如n_samples /（n_classes * np.bincount（y））。请注意，如果指定了sample_weight，这些权重将与sample_weight（通过fit方法传递）相乘。\n",
    "\n",
    "random_state:int,RandomState实例或None，可选，默认值：None\n",
    "\n",
    "在随机数据混洗时使用的伪随机数生成器的种子。 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random使用的RandomState实例。 在求解器=='sag'或'liblinear'时使用。\n",
    "\n",
    "solver:{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}\n",
    "\n",
    "默认: ‘liblinear’ 在优化问题中使用的算法。\n",
    "\n",
    "对于小型数据集，'liblinear'是一个不错的选择，而'sag'和'saga'对于大型的更快。\n",
    "\n",
    "对于多类问题，只有'newton-cg'，'sag'，'saga'和'lbfgs'处理多项损失; 'liblinear'仅限于’ovr’方案。'newton-cg'，'lbfgs'和'sag'只处理L2惩罚，而'liblinear'和'saga'处理L1惩罚。请注意，“sag”和“saga”快速收敛仅在具有大致相同比例的要素上得到保证。 您可以使用sklearn.preprocessing中的缩放器预处理数据。\n",
    "\n",
    "max_iter: int 默认:100 仅适用于newton-cg，sag和lbfgs求解器。 求解器收敛的最大迭代次数。\n",
    "\n",
    "muti_class:str,{‘ovr’:’multinomial’},默认:’ovr’\n",
    "\n",
    "多类选项可以是'ovr'或'multinomial'。 如果选择的选项是'ovr'，那么二元问题适合每个标签。 另外，最小化损失是整个概率分布中的多项式损失拟合。 不适用于liblinear解算器。\n",
    "\n",
    "verbose: int,默认:0 对于liblinear和lbfgs求解器，将verbose设置为任何正数以表示详细程度。\n",
    "\n",
    "warm_start:bool 默认:False\n",
    "\n",
    "设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。 对于liblinear解算器没用。\n",
    "\n",
    "版本0.17中的新功能：warm_start支持lbfgs，newton-cg，sag，saga求解器。\n",
    "\n",
    "n_jobs: int,默认:1\n",
    "\n",
    "如果multi_class ='ovr'“，则在对类进行并行化时使用的CPU核心数。 无论是否指定'multi_class'，当``solver``设置为'liblinear'时，都会忽略此参数。 如果给定值-1，则使用所有核心。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习部分：\n",
    "利用sklearn解决分类问题\n",
    "sklearn.linear_model.LogisticRegression\n",
    "利用梯度下降法将相同的数据分类，画图和sklearn的结果相比较\n",
    "利用牛顿法实现结果，画图和sklearn的结果相比较，并比较牛顿法和梯度下降法迭代收敛的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model,datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zelon\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Zelon\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e2)\n",
    "logreg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = logreg.predict_proba(X_test_std)\n",
    "acc = logreg.score(X_test_std,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5777777777777777\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
