{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论部分\n",
    "决策树是一种树型结构的机器学习算法,它每个节点验证数据一个属性,根据该属性进行分割数据,将数据分布到不同的分支上,直到叶子节点,叶子结点上表示该样本的label. 每一条从根节点到叶子节点的路径表示分类[回归]的规则. \n",
    "特征选择：\n",
    "信息熵：是度量样本集合纯度最常用的指标，假定当前样本集合D中第k类样本所占的比例为$p_k$(k = 1,2,...,|$\\gamma$|),则D的信息熵定义为\n",
    "$$Ent(D)=-\\sum^{|\\gamma|}_{k=1}p_klog_2p_k         ----（1）$$\n",
    "Ent(D)的值越小，则D的纯度越高。\n",
    "信息增益：假定离散属性a有V个可能的取值{${a^1,a^2,...,a^V}$},若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有属性a上取值为$a^v$的样本，记为$D^v$.根据公式（1）计算的信息熵，再考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重|$D^v$|/|D|,即样本数越多的分支节点的影响越大，于是可计算出用属性a对杨本集进行划分所获得的信息增益越大。\n",
    "$$Gain(D,a)=Ent(D)-\\sum^V_{v=1}\\frac{|D^v|}{D}Ent(D^v)    ------(2)$$\n",
    "增益率：\n",
    "$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(D)}  ------(3)$$\n",
    "其中\n",
    "$$IV(a)=-\\sum^V_{v=1}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}   ----(4)$$\n",
    "称为属性a的固有值，属性a的可能取值数目越多（V越大），则IV（a）的值越大。\n",
    "基尼系数：CART决策树使用基尼系数来选择划分属性，例如数据集D的纯度可用基尼值来度量：\n",
    "$$Gini(D)=\\sum^{|\\gamma|}_{k=1} \\sum_{k\\prime \\neq k} p_kp_k{\\prime}$$\n",
    "$$=1-\\sum^{|\\gamma|}_{k=1}p^2_k    ---- (5)$$\n",
    "Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini（D）越小，则数据集D的纯度越高。属性a的基尼系数定义为：\n",
    "$$Gini_index(D,a)=\\sum^V_{v=1}\\frac{|D^v|}{|D|}Gini(D^v)   -----   (6)$$\n",
    "\n",
    "决策树生成：\n",
    "ID3决策树:ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定集合的测试属性对被选取的测试属性创建一个节点，并以该节点的属性标记，对该属性的每个值创建一个分支据此划分样本。\n",
    "C4.5决策树：在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好了决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。决策树的优势在于不需要任何领域知识或参数设置，适合于探测性的知识发现。\n",
    "\n",
    "CART决策树:CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。\n",
    "　　假设K个类别，第k个类别的概率为pk，概率分布的基尼系数表达式：\n",
    "$$Gini(p)=\\sum^K_{k=1}p_k(1-p_k) = 1 - \\sum^K_{k=1}p^2_k   ----   (7)$$\n",
    "\n",
    "决策树剪枝\n",
    "剪枝类型：预剪枝、后剪枝\n",
    "\n",
    "预剪枝：在构造决策树的同时进行剪枝。所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。\n",
    "后剪枝是在决策树生长完成之后，对树进行剪枝，得到简化版的决策树。\n",
    "剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。后剪枝是目前最普遍的做法。\n",
    "后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则(majority class criterion)确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识,所标识的类称为majority class ，（majority class 在很多英文文献中也多次出现）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn参数详解\n",
    "DecisionTreeClassifier: sklearn中多分类决策树的接口.\n",
    "\n",
    "Paramters:\n",
    "\n",
    "criterion : str, 可选参数(default=\"gini\")\n",
    "    这个参数表示使用什么度量划分的质量. gini: 表示使用基尼指数.\n",
    "    \n",
    "    entropy: 表示使用的是信息增益.\n",
    "\n",
    "splitter : str, optional(default=\"best\")\n",
    "    选择分割节点的策略. 支持最优(best)和随机(random)两种方式.\n",
    "\n",
    "\n",
    "max_depth : int or None, optional(dafault=None)\n",
    "    表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,\n",
    "    直到叶子节点的个数小于min_samples_split个数.\n",
    "\n",
    "min_samples_split : int, float, optional(default=2)\n",
    "    表示最小分割样例数.\n",
    "    if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.\n",
    "    if float, 表示的比例[0,1],最小的分割数字=ceil(min_samples_split * n_samples)\n",
    "\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "    表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.\n",
    "    if int, min_samples_leaf就是最小样例数.\n",
    "    if float, 表示的比例[0,1],最小的样例数=ceil(min_samples_leaf * n_samples)\n",
    "\n",
    "\n",
    "min_weight_fraction_leaf : float, optional (default=0.) \n",
    "\n",
    "\n",
    "max_features : int, float, str or None, optional(default=None)\n",
    "    进行最优分割时,特征的最大个数.\n",
    "    if int, max_features是每次分割的最大特征数\n",
    "    if float, int(max_features * n_features)作为最大特征数\n",
    "    if \"auto\", 则max_features=sqrt(n_features)\n",
    "    if \"sqrt\", 则max_features=sqrt(n_features)\n",
    "    if \"log2\", 则max_features=log2(n_features)\n",
    "    if None, 则max_features=n_features\n",
    "\n",
    "\n",
    "random_state : int, RandomState instance or None, optional (default=None)\n",
    "    随机化种子, if None,使用np.random随机产生\n",
    "\n",
    "\n",
    "max_leaf_nodes : int or None, optional (default=None)\n",
    "    最大的叶子节点个数,如果大于这个值,需要进行继续划分. None则表示没有限制.\n",
    "\n",
    "\n",
    "min_impurity_decrease : float, optional (default=0.)\n",
    "    分割之后基尼指数大于这个数,则进行分割.\n",
    "    N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                    - N_t_L / N_t * left_impurity)\n",
    "\n",
    "\n",
    "min_impurity_split : float, default=1e-7\n",
    "    停止增长的阈值,小于这个值直接返回.\n",
    "\n",
    "\n",
    "DecisionTreeRegressor: sklearn中回归树的接口.\n",
    "\n",
    "criterion : str, optional (default=”mse”)\n",
    "    其他参数和分类树类似.\n",
    "    mse: mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node,\n",
    "    mae: mean absolute error, which minimizes the L1 loss using the median of each terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用sklearn解决分类问题和回归预测。\n",
    "sklearn.tree.DecisionTreeClassifier   分类\n",
    "sklearn.tree.DecisionTreeRegressor   回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "X_train,X_Test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier Score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"classifier Score:\",clf.score(X_Test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
